import streamlit as st
import tempfile
import os
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# ==========================================
# ğŸ¨ 1. å‰ç«¯é…ç½® (HTML/CSS)
# ==========================================
st.set_page_config(page_title="Mini-Brain Pro", layout="wide")

# æ³¨å…¥è‡ªå®šä¹‰ CSS (è£…ä¿®å¤§å ‚)
# æˆ‘ä»¬æŠŠèŠå¤©æ°”æ³¡çš„å­—ä½“æ”¹å¤§ä¸€ç‚¹ï¼ŒèƒŒæ™¯è‰²å¾®è°ƒ
st.markdown("""
<style>
    .stChatMessage {
        font-family: 'Helvetica Neue', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§  Mini-Brain Pro: æœ‰è®°å¿†çš„ç¬¬äºŒå¤§è„‘")

# ==========================================
# ğŸ§  2. åç«¯é€»è¾‘ï¼šçŠ¶æ€ç®¡ç† (Session State)
# ==========================================

# åˆå§‹åŒ–èŠå¤©è®°å½• (æµ·é©¬ä½“)
# å¦‚æœå†…å­˜é‡Œæ²¡æœ‰ "messages"ï¼Œå°±æ–°å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state.messages = []

# åˆå§‹åŒ– RAG é“¾ (å¤§è„‘)
# æˆ‘ä»¬æŠŠå¤§è„‘ä¹Ÿå­˜åœ¨ session_state é‡Œï¼Œé˜²æ­¢æ¯æ¬¡ç‚¹æŒ‰é’®éƒ½é‡è½½æ¨¡å‹ï¼ˆå¤ªæ…¢ï¼‰
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# ==========================================
# âš™ï¸ 3. ä¾§è¾¹æ ï¼šé…ç½®ä¸æ•°æ®å¤„ç†
# ==========================================
with st.sidebar:
    st.header("ğŸ”§ é…ç½®æ§åˆ¶å°")
    api_key = st.text_input("API Key", type="password")
    base_url = st.text_input("Base URL", value="https://api.deepseek.com")
    
    st.divider()
    
    uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  PDF å–‚ç»™å¤§è„‘", type="pdf")
    
    # å½“ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶åï¼Œè§¦å‘æ•°æ®å¤„ç†æµæ°´çº¿
    if uploaded_file and api_key and not st.session_state.qa_chain:
        with st.spinner("æ­£åœ¨åˆ‡ç‰‡ã€å‘é‡åŒ–ã€å­˜å…¥æ•°æ®åº“..."):
            # A. ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(uploaded_file.getvalue())
                tmp_path = tmp.name
            
            # B. åŠ è½½ä¸åˆ‡åˆ†
            loader = PyPDFLoader(tmp_path)
            docs = loader.load()
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
            splits = splitter.split_documents(docs)
            
            # C. å‘é‡åŒ– (Embeddings)
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
            
            # D. å­˜å…¥ Chroma (Vector Store)
            vectorstore = Chroma.from_documents(splits, embeddings)
            
            # E. ç»„è£…å¤§è„‘
            llm = ChatOpenAI(
                model="deepseek-chat", 
                api_key=api_key, 
                base_url=base_url,
                temperature=0
            )
            
            # F. ä¿å­˜åˆ°åç«¯çŠ¶æ€é‡Œ
            prompt = ChatPromptTemplate.from_messages([
                ("system", "Use the given context to answer the question. If you don't know the answer, say you don't know. Use three sentence maximum and keep the answer concise. Context: {context}"),
                ("human", "{input}"),
            ])
            question_answer_chain = create_stuff_documents_chain(llm, prompt)
            st.session_state.qa_chain = create_retrieval_chain(
                vectorstore.as_retriever(), question_answer_chain
            )
            
            st.success("âœ… å¤§è„‘å·²æ¿€æ´»ï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†ã€‚")
            os.remove(tmp_path) # æ¸…ç†ä¸´æ—¶æ–‡ä»¶

# ==========================================
# ğŸ’¬ 4. ä¸»ç•Œé¢ï¼šèŠå¤©å¾ªç¯ (Event Loop)
# ==========================================

# A. æ¸²æŸ“å†å²æ¶ˆæ¯ (æŠŠåç«¯å†…å­˜é‡Œçš„ä¸œè¥¿ç”»åˆ°å‰ç«¯é¡µé¢ä¸Š)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# B. ç›‘å¬ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("å‘ä½ çš„æ–‡æ¡£æé—®..."):
    
    # 1. æ£€æŸ¥æœ‰æ²¡æœ‰æ¿€æ´»å¤§è„‘
    if not st.session_state.qa_chain:
        st.error("è¯·å…ˆåœ¨å·¦ä¾§é…ç½® API å¹¶ä¸Šä¼ æ–‡ä»¶ï¼")
        st.stop()

    # 2. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ (å‰ç«¯)
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # 3. è®°å…¥å†å² (åç«¯)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 4. è°ƒç”¨ AI æ€è€ƒ
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            # è°ƒç”¨ RAG é“¾
            response = st.session_state.qa_chain.invoke({"input": prompt})
            result = response["answer"]
            st.markdown(result)
            
    # 5. è®°å…¥å†å² (åç«¯)
    st.session_state.messages.append({"role": "assistant", "content": result})